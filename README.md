# Comparing performance between Machine Learning Models 2
Comparing performance between machine learning models decision trees, random forests, ensemble learning methods, PCA-reduced models, and neural networks for a dataset of ~4000 observations, where model.py is the compiled code.

I compare the effectiveness between using decision trees, random forests, gradient boosting, XGBoosting, PCA, and neural networks. The decision trees are pre-and-post-pruned using max_depth (depth of the tree) and ccp-alpha (cost-complexity pruning, where greater values correspond to greater number of nodes pruned), and the PCA is done for both a 95% explained variance ratio and a 98% ratio before applying the best performing model from previous parts of the process. The solvers for the neural networks here are the Adam and SGD solvers. These models are applied on the Abalone dataset from the UC Irvine Machine Learning Repository (https://archive.ics.uci.edu/dataset/1/abalone). Despite the dataset being more suited for a regression problem, for data preprocessing practice, this dataset is treated as a multiclass classification problem. Evaluating the effectiveness then requires performance metrics suited for classification problems. As such, I use accuracy and F1 score to compare the performances between the models developed, F1 score in particular to account for class imbalance.

Using the means and standard deviations of 10 experimental runs for each model with a different 60% training split in each run, the neural network using the Adam solver had the best performance with accuracy 0.6477 ± 0.0101 and F1 score 0.6399 ± 0.0113. In comparison, the PCA-reduced gradient boosting model performed the weakest with accuracy 0.5776 ± 0.0119 and F1 score 0.5461 ± 0.0134. This suggests that neither of these models were suitable for the multiclass classification and more advanced models may be needed for better results. As the models were generally untuned (most parameters were left on default settings), the study provides a basis upon which one can understand the differences in models, particularly with the way the process was built in a hierarchical manner. Thus, easy comparisons can be made with future developments of the models, either in terms of hyperparameter tuning or data pre-processing.

This study serves as an overview of a wide variety of common and foundational classification models, exploring decision trees, random forests, ensemble learning methods gradient boosting and XGBoosting, PCA-transformed gradient boosting and neural networks unregularised and regularised. Modelling on the Abalone dataset with response variable divided into 4 classes, the Adam neural network developed through scikit-learn had the best performance, despite its weak accuracy and F1 score. On the other hand, reducing the dataset through PCA resulted in the weakest results. Most models had quite good generalisability but requires more finer parameter tuning for a better representation of the robustness of the model, and could be explored in future research - comparing the models with the best parameter tuning for each. With the overall weak performance of the models considered, it may be that the division of the response variable into 4 classes was not a suitable number of categories to apply the models on, and could be explored further too.
